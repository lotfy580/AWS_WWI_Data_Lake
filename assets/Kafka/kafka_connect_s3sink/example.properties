name=s3-sink-connector
connector.class=io.confluent.connect.s3.S3SinkConnector
tasks.max=3  # Adjust based on the number of partitions and resources
topics=<your_topic_list>  # Comma-separated list of your 30 topics
s3.region=<your_aws_region>
s3.bucket.name=<your_s3_bucket_name>
s3.part.size=5242880  # 5MB, adjust based on your needs
s3.compression.type=gzip  # Optional: Choose compression type (none, gzip, snappy, etc.)

# Record Key and Value Converters
key.converter=org.apache.kafka.connect.storage.StringConverter
value.converter=io.confluent.connect.avro.AvroConverter
value.converter.schema.registry.url=<schema_registry_url>

# S3 Partitioner settings
partitioner.class=io.confluent.connect.storage.partitioner.TimeBasedPartitioner
partition.duration.ms=3600000  # 1 hour in milliseconds
path.format='year'=YYYY/'month'=MM/'day'=dd/'hour'=HH  # Partition by year/month/day/hour
locale=en_US  # Locale setting, adjust if needed
timezone=UTC  # Timezone setting, adjust if needed

# Time-based partitioner requires this setting
timestamp.extractor=Record
timestamp.field=<timestamp_field>  # Field in the record to use as timestamp

# File rotation settings
rotate.schedule.interval.ms=3600000  # Rotate files every hour
rotate.interval.ms=3600000  # Also triggers a file rotation if this interval is reached
flush.size=10000  # Flush after this many records

# Other settings
storage.class=io.confluent.connect.s3.storage.S3Storage
format.class=io.confluent.connect.s3.format.avro.AvroFormat  # Store records in Avro format
schema.compatibility=BACKWARD  # Schema compatibility, adjust as needed
